\RequirePackage{lineno}
\documentclass[twocolumn]{article}
\usepackage{cite}
\usepackage{graphicx} % Required for inserting images
\usepackage{color}
\usepackage{lineno}
\usepackage[affil-it]{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{abstract} % needed for custom author footnotes/thanks
\usepackage{hyperref}

\newcommand{\nxn}[1]{\ensuremath{#1 \times #1}}
\newcommand{\bra}[1]{\ensuremath{\left\langle #1 \right|}}
\newcommand{\ket}[1]{\ensuremath{\left| #1 \right\rangle}}
\newcommand{\braket}[2]{\ensuremath{\left\langle #1 \middle| #2 \right\rangle}}
\newcommand{\ketbra}[2]{\ensuremath{\left| #1 \right\rangle\left\langle #2 \right|}}


\newcommand{\MSU}{Department of Physics and Astronomy, Michigan State University, East Lansing, Michigan 48824}
\newcommand{\CMSE}{Department of Computational Mathematics, Science and Engineering, Michigan State University, East Lansing, Michigan 48824}
\newcommand{\FRIB}{Facility for Rare Isotope Beams, East Lansing, Michigan 48824}
\newcommand{\Oslo}{Department of Physics and Center for Computing in Science Education, University of Oslo, N-0316 Oslo, Norway}
\newcommand{\Cornell}{Department of Electrical and Computer Engineering, Cornell Tech, New York, NY 10044}
\begin{document}

%\linenumbers
\title{Matrix Model}

\author[1,2,3]{Patrick Cook\footnote{Contributed equally}\thanks{\texttt{cookpat4@msu.edu}; Corresponding author}}

% the following disaster seems to be the only good way to get the same * footnote with two corresponding authors
\newcommand\CoAuthorMark{\addtocounter{footnote}{-1}\footnotemark[\arabic{footnote}]\addtocounter{footnote}{1}}

\author[1,2,3]{Danny Jammooa\protect\CoAuthorMark\thanks{\texttt{jammooa@frib.msu.edu}; Corresponding author}}

\author[1,2,4]{Morten~Hjorth-Jensen}

\author[5]{Daniel~D.~Lee}

\author[1,2]{Dean~Lee}

\author[5]{\dots}

\affil[1]{\MSU}
\affil[2]{\FRIB}
\affil[3]{\CMSE}
\affil[4]{\Oslo}
\affil[5]{\Cornell}

\date{\today} % it's always \today, today

\twocolumn[
  \begin{@twocolumnfalse}
    \maketitle
    \begin{abstract}
        This paper presents a novel implicit deep learning architecture called Matrix Model (MM). Stemming from the MM concept, we crafted two separate data-driven dimensionality reduction emulators to address specific challenges within physics. Emulators have the potential of bypassing the computational burdens of complex scientific calculations without compromising accuracy.  Matrix Eigenvalue Emulator (MEE), a form of Machine Learning that originally was designed to leverage the structure of Eigenvector Continuation (EC), enables fast calculation of extremal eigenvalues, other observables, and identification of branch points without the explicit formation of norm and Hamiltonian matrices.  MEE's performance is validated against EC and other interpolation techniques using two distinct nuclear physics models. Then MEE is also tested as a general Machine Learning method. Meanwhile, the Matrix Unitary Emulator (MUE) serves as a post-processing technique aimed at diminishing Trotter errors in Quantum Computers, with its efficacy demonstrated on the Heisenberg spin chain model. Our findings highlight that both MEE and MUE as data-driven dimensionality reduction methods excel or match their counterparts. The results offer valuable insights for further development and potential applications of MM in nuclear physics and beyond.
    \end{abstract}
  \end{@twocolumnfalse}]

%########################################
\section{Introduction}
A common challenge faced today in many fields of physics is the issue of computational complexity for the problems being studied. While advancements in Quantum Computing (QC) present a promising avenue, we are currently in the Noisy Intermediate-Scale Quantum (NISQ) computing era. It's only in recent times that evidence has emerged, suggesting that NISQ quantum computers might simulate systems beyond the capability of classical methods \cite{kim2023evidence}. Therefore, seeking progress in classical computation remains valuable, even in the presence of fault-tolerant QC.
\par
Thus we have developed a novel implicit deep learning architecture \cite{bai2020multiscale,el2021implicit,kawaguchi2021theory} called Matrix Model (MM). Stemming from the MM concept, we crafted two separate data-driven dimensionality reduction emulators to address specific challenges within physics. Where emulators have the potential of bypassing the computational burdens for complex scientific calculations without compromising accuracy.
\par
Currently a challenge in quantum physics is finding the extremal eigenvalues and eigenvectors of a Hamiltonian matrix too large to store in computer memory \cite{DFrame,ASarkar}. There are numerous efficient methods developed for this task. All existing methods either use Monte Carlo simulations, diagrammatic expansions, variational methods, or some combination. The problem is that they generally fail when some control parameter in the Hamiltonian matrix exceeds some threshold value. Recently, the role of emulators, algorithms that provide fast yet accurate approximations to costly computations, has become increasingly crucial in addressing these computational challenges \cite{Phillips_2021,Demol_2020}.
\par
In nuclear physics extremal eigenvalues of a Hamiltonian matrix are calculated using eigenvector continuation (EC) and related subspace projection methods by solving the generalized eigenvalue problem \cite{DFrame,ASarkar,SKONIG,PhysRevC.106.054322}. 
One of the emulators we crafted from MM is called Matrix Eigenvalue Emulator (MEE), which utilize the structure of eigenvector continuation without requiring the explicit construction of norm and Hamiltonian matrices.
\par 
In the NISQ era, to simulate the dynamics of a quantum system on a QC, one has to construct a sequence of computational gates that implement 
\begin{align}
    U = e^{-iHt}
\end{align}
Most Hamiltonians, often constitute the sum of $k$ local Hamiltonians $H_k$. When the $H_k$ don't commute, one can't apply the Lie product formula to convert a unitary of sums of $H_k$ to a product of unitaries. Thus often times, one has to apply a truncation called Trotterization, which is a popular method for simulating non-commuting Hamiltonians on QC.
\begin{equation}
    e^{-iHt} = (e^{-iH_0t/r} e^{-iH_1t/r} \cdots)^r
\end{equation}
Such truncation causes simulation errors, which can be constrained by a error \(\epsilon\), defined as:
\begin{equation}
    ||e^{-iHt} - U|| \le \epsilon
\end{equation}
However, with the current state of NISQ quantum computers, attempting small rotations of \(dt\) often results in a challenge of extensive circuit depth. This means that as Trotter errors reduce, gate errors augment due to decoherence\cite{childs2021theory}.
The 2nd emulator that we developed called Matrix Unitary Emulator (MUE) is a post processing error mitigation technique that tackles Trotter Error. 
\par
This paper is organized into a series of sections. The second section introduces the Matrix Model (MM), with a description of both Matrix Model Emulators (MEEs) and Matrix Unitary Emulator (MUE). In the third section, we present our results for both MEE and MUE. The third section expands the application of MEEs by considering it as general machine learning model.  Finally, the fourth section encapsulates our thoughts from the study. 

%########################################
\section{Matrix Model}
The most general form of Matrix Model (MM) is a complex Hermitian matrix whose coefficients are analytic functions of a set of parameters $M(\Vec{c})$. Where we learn the matrix elements of $M(\Vec{c})$, by fitting its eigenvalues to a given function. 
\subsection{Matrix Eigenvalue Emulator}
  Here we consider the simplest case of the family of MMs called Matrix Eigenvalue Emulator (MEE) parameterized by a single variable $c$ of the form
\begin{equation}
\begin{aligned}
\label{MEE}
    M(c) = A + cB
\end{aligned}
\end{equation}
Where $c$ is a scalar parameter. These coefficients are determined by minimizing the difference between the $kth$ eigenvalue of $M(c)$ and true value of selected training points. Explicitly the training data/input available to MEEs is the set of triplets of a value of $c$, the index of the MEEs eigenvalue $k$, and the value of true $f(c)$.
\begin{equation}
    \begin{aligned}
        \{c,k,f(c)\}
    \end{aligned}
\end{equation}

The a family of matrices that form $M(c)\in\mathbb{C}^{\nxn{n}}$, $A$ and $B$ are given by

\begin{equation}
\label{AB}
    \begin{aligned}
        {\scriptsize
    A= \begin{bmatrix}
        a_1 & 0 & 0 & 0 \\
        0 & \ddots & 0& 0  \\
        0 & 0 & \ddots& 0   \\
        0 & 0& 0  & a_n  \\
    \end{bmatrix},
    B = \begin{bmatrix}
        b_{11}& b_{12}&\cdots &b_{1n}  \\
        b_{12}^*&\ddots&&b_{2n}\\
        \vdots& &\ddots&\vdots\\
        b_{1n}^*&b_{2n}^*&\cdots&b_{nn}
        \end{bmatrix}
    }
    \end{aligned}
\end{equation}

The eigenvalues of $M(c)$ satisfy the characteristic polynomial
\begin{equation}
\label{cp}
    \begin{aligned}
        det(M(c)-\lambda)=0
        \end{aligned}
\end{equation}
This is an algebraic equation in $\lambda$ of degree $n$, with coefficients which are holomorphic in $c$, excluding some distinct values of $c$ known as exceptional points. The roots of Eq.~\ref{cp} constitutes one or several analytic functions i.e. the eigenvalues of $M(c)$.
\par 

When we investigate the behavior of the eigenvalues around an exceptional point, we can assume, without losing generality, that the exceptional point is at $c=0$. Exceptional points within the complex plane correspond to an avoided level-crossing on the real-axis. The eigenvalues of $M(c)$ for $c\in D$, where $D$ is a finite region of the Riemann surface. As $D$ is continuously rotated around $c=0$, the eigenvalues maintain their analytic continuity. After a complete rotation around $c=0$, the eigenvalues undergo a permutation amongst themselves \cite{Kato:1966:PTL,DFrame,ASarkar}.
\par
Lets consider the avoided level-crossing between two eigenvalues $\lambda_1(c)$ and $\lambda_2(c)$. Assume these two eigenvalues converge at $c=0$ in the complex plane. Consequently, the two eigenvalues are equal, as in $\lambda_1(c=0)=\lambda_2(c=0)$. Therefore, at $c=0$,  only a single eigenvalue with a single corresponding eigenvector is present. These two eigenvalues represent the values of one analytic function on two different Riemann sheets, and these sheets converge at the branch point at $c=0$.  Now lets consider a series expansion of the eigenvalues around $c=0$:
\begin{equation}
    \begin{aligned}
        E_j(c) = \sum_{n=0}^{\infty} \frac{1}{n!}E_j^{(n)}(0)c^n
    \end{aligned}
\end{equation}
The presence of the branch point limits the convergence area of the series expansions, which leads to the failure of the series expansion for large values of $c$. This limitation arises as the series expansion fails to capture the change in character around an instance of avoided level-crossing due to the permutation of eigenvalues.
\par 
As follows, MEEs fits the coefficients of the roots of the characteristic polynomial Eq.~\ref{cp}, and as a result don't encounter the same pitfalls as a series expansion, thereby enabling MEE to identify branch points. Also, since the roots of the characteristic polynomial form a polynomial, the Weierstrass approximation theorem holds for MEEs. As a consequence, MEEs can provide an efficient and natural framework for learning manifolds that are well approximated by algebraic structures.
\begin{figure}[h!]
    \centering
\includegraphics[width=0.7\linewidth]{Figures/MMEFlow.png}
    \caption{MEE training flow chart}
    \label{fig:MEE_flow}
\end{figure}
We also note that MEE has the capability to learn and compute additional observables $\hat{O}$.  
\begin{equation}
\begin{aligned}
    \langle \hat{O}\rangle = \bra{\psi(c)}\hat{O}\ket{\psi(c)}
\end{aligned}
\end{equation}
However, in order to learn $\hat{O}$, it is necessary to have already learned $M(c)$ to have access to the eigenstates $\ket{\psi(c)}$.
A single MEE can be used to learn multiple observables for a given system. The diagram presented in Figure~\ref{fig:MEE_flow} illustrates the procedure of the MEE protocol for the learning of a matrix $M(c)$, along with its applicability in learning other observables and identifying branch points.
%########################################
\subsection{Matrix Unitary Emulator}
Here we consider another case of the family of MMs called Matrix Unitary Emulator (MUE), where MUE is a product of matrix exponentials parameterized by a single variable $dt$ of the form
\begin{align}
\label{MUE}
    M(dt) = e^{-iAdt}e^{-iBdt}
\end{align}
Where $dt$ is a scalar parameter. $A$ and $B$ are described by Eq.~\ref{AB}, and the MUE generaly follows the same minimization procedure as MEE with the training data avalible to MUE:
\begin{equation}
    \begin{aligned}
        \{dt,k,f(dt)\}
    \end{aligned}
\end{equation}
However, the energies $E_k$ of MUE is determined by setting
$e^{-iE_kdt}$ equal to the corresponding eigenvalue for the Trotterized time evolution operator $e^{-iAdt}e^{-iBdt}$ resulting in
\begin{align}
\label{En}
    E_k = \frac{ln(\lambda_k(e^{-iAdt}e^{-iBdt})}{-idt}
\end{align}
Once an MUE is trained, it can be used to extrapolate to $dt\rightarrow 0$.
%########################################
\section{Results}
\subsection{Matrix Eigenvalue Emulator}
%########################################
Many-body systems are inherently complex, as they involve a large number of interacting particles that can give rise to non-perturbative behavior. This means that standard perturbation techniques, may not be suitable for accurately describing the system’s behavior in certain regimes. In the case of an avoided level crossing, the many-body corrections fail to track the change in character of the energy levels, which signifies that more sophisticated methods are needed to properly describe the system’s behavior in such instances. 
\par
Matrix Eigenvalue Emulators (MEEs), can be perceived as the process of learning an effective Hamiltonian in a reduced dimensional space. Where we learn the MEEs given data only of energies as a function of a parameter $c$. 

\par 
Unlike perturbation techniques, which are known to be unsuccessful in regions of avoided level crossings. By learning an effective Hamiltonian, MEEs is able to interpolate in the region where avoided level crossing occur by tracking the changes in character. In this section we demonstrate the result for the Lipkin-Meshkov-Glock (LMG) model in the Total Spin Basis (TSB) \ref{TBS}, and for the Anharmonic Oscillator.
\begin{align}
    H = w(a^\dagger a) + g (a^\dagger + a) ^ 4
\end{align}
\par

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/lmg1000.png}
    \caption{Minimum eigenvalue of a $\nxn{5}$ MEE trained on ground state energies of N=1000 TSB LMG, and learning $\langle\hat{O}\rangle$.}
    \label{fig:obv}
\end{figure}
Figure~\ref{fig:obv} compares the result of N=1000 TSB LMG model with a $\nxn{5}$ MEE and EC, where the MEE was trained on the ground state energy around avoided level crossing. We note that given only information of the eigenvalues MEE produced results equivalent to EC.  In Figure~\ref{fig:obv}, we also note that the learned $M(c)$ was used to learn $\langle Sx^2\rangle/N$. 
\par
In some instances, where the wavefucntion is just a tensor product of every qubit
\begin{align}
    H=-\sum_i^N\sigma^z_i-c\sum_i^N\sigma_i^x
\end{align}
~\ref{fig:2x2_TSB_LMG}, MEE successfully discovers a lower-dimensional representation that precisely replicates the ground state energy of the original Hamiltonian, where EC fails to identify such representation. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/2x2_mee_ec2.png}
    \includegraphics[width=0.4\textwidth]{Figures/2x2_mee_ec3.png}
    \caption{Trained $\nxn{2}$ MEE and $\nxn{5}$ EC for a Hamiltonian $H=-\sum_i^N\sigma^z_i-c\sum_i^N\sigma_i^x$ at $N=1000$}
    \label{fig:2x2_TSB_LMG}
\end{figure}
Especially in the instance where the training points were taken away from the avoided-level crossing, EC was unable to extrapolate further, where MEE was successfully. These findings indicate that MEE has the ability to project to a lower-dimensional space in certain instances, where other methods like EC and reduced basis techniques are unable to do so. MEE application to identifying branch points can be seen in \ref{BPA}. The results in this section informs us that eigenvalues even though are scalars encodes more information then previously believed. Given that MEEs eigenvectors can be used to learn other $\langle O\rangle$, and calculate Berry's phase, when considering that MEEs are only trained on eigenvalues.

%#######################################

%########################################
\subsection{Matrix Unitary Emulator}
The dynamics of a quantum system is dictated by the Schrödinger equation:
\begin{equation}
    i\hbar\frac{d}{dt}|\psi(t)\rangle = H|\psi(t)\rangle
\end{equation}
The solution, given by 
\begin{equation}
    |\psi(t)\rangle = e^{iHt}|\psi(0)\rangle
\end{equation}
outlines the system's state after a Hamiltonian operates on it over a specific duration. The challenge of Hamiltonian simulation is that for known Hamiltonian \(H\) and a time evolution \(t\), produce a sequence of computational gates that implements
\begin{equation}
    U = e^{iHt}
\end{equation}
However, due errors that arise from truncation (Trotter error), gate error, decoherence, ect. Quantum computers today are subject to multitude amount of noisy. In recent years, classical post-processing techniques known as error mitigation attempts to tackle different types of error. Techniques range from Unfolding~\cite{leymann2020bitter} for measurement error, Zero-noise extrapolation\cite{giurgica2020digital} for decoherence, and polynomial fitting \cite{rendon2023improved} for Trotter error.
\par
Here we show results for MUE through classical calculation of the Heisenberg Model\cite{smith2019simulating},\ref{heisen}. Where we learn Eq.~\ref{MUE} by fitting its ground state energy to the ground state energy of the Heisenberg Model for different time step $dt$.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.55\textwidth]{Figures/2x2_MUE_7_Heisen_xyz_dt_0.1_0.2.png}
    \caption{Learning $\nxn{2}$ MUE on ground state energy training data from $U(dt)=e^{-iHdt}$  where $H=\sum_i^N \sigma^z_i + \sum_i^N(\sigma^x_i\sigma^x_{i+1} -0.5\sigma^y_i\sigma^y_{i+1}+\sigma^z_i\sigma^z_{i+1})$}
    \label{fig:MUE}
\end{figure}
As shown in Figure~\ref{fig:MUE}, MUE successfully found a $\nxn{2}$ unitary representation of the original higher dimensional unitary, and  extrapolated to low $dt$ better then a polynomial of degree $4$. 
%########################################
\section{MEE: A General Interpolant}
%%%%%%%%%%%%%%%%%%%%%%%%%%%Rewrite%%%%%%%%%%%%%%%%%%%%%
Perturbative expansions serve as a powerful tool within theoretical physics. However, their efficiency is often hindered due to the emergence of non-convergent series, such as asymptotic series resulting from weak coupling perturbation theory in quantum field theory. Often, multiple theoretical models can describe a single system, giving access to two perturbative expansions of physical quantities at two distinct points within the parameter space\cite{honda2014perturbation,semposki2022interpolating,Ekström_2019}. Examples of such are the low and high temperature expansions in statistical systems, or the weak and strong coupling expansions in lattice gauge theory.
\par
However, these expansions fail to provide accurate information about the physical quantity in the intermediate region situated between the two points. The standard method physicists employ is an interpolation function, Pade approximants, to interpolate between these limits. In this section, we examining MEEs as general interpolation function, where the available training data is exclusively from these extreme scenarios. We then interpolate the intermediate region and draw a comparison with Pade approximation. 
\par
A demonstration of this comes from \cite{honda2014perturbation,semposki2022interpolating}, where the true model examined is a zero-dimensional $\phi^4$ theory partition function dependent on a parameter $g$ and expand around $g=0$ and $1/g=0$. They utilize these expansions around small and large values of $g$ as their training data for their interpolation function. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/BMM_vs_5x5MEE.png}
    \caption{Overlay of the $\nxn{5}$ MEE for Ns, Nl = 5 case with the mixed model results using BMM. Inset: A close up of the central mixing
region}
    \label{fig:BMM}
\end{figure}
We adopted the same training approach to train the innermost eigenvalue of a $\nxn{5}$ MEE and interpolate the 'gap' region. Note, training was done on the innermost eigenvalue due to the extremal eigenvalues having convexity constraints.
\par
The result of the comparison is shown in Figure~\ref{fig:BMM}, where within the central mixing region, the MEE demonstrates significant agreement with the exact solution.

%#################################################

%#####################################
\section{Conclusion}
Throughout this paper, we have demonstrated the diverse capabilities of the Matrix Model (MM). With a primary example of Matrix Eigenvalue Emulators (MEEs) and Matrix Unitary Emulators (MUEs), which data-driven dimensionality reduction methods. 
\par 
Where MEEs learns an effective Hamiltonian based solely on eigenvalue data. Capable of interpolation within challenging regions, such as avoided level crossings, and enables the identification of branch points within the complex plane. 
Our research further illustrates that MEEs possess the capacity to learn other observables, and even identifying lower dimensional spaces which had proven elusive to other subspace projection methods. With the hope that MEEs gets adopted as a data driven Emulator by the quantum physics community \cite{Demol_2020,DRISCHLER2021136777,FURNSTAHL2020135719,PhysRevC.103.014612}. Moreover, MEEs have been shown to serve as general machine learning method, capable of approximating certain complex, yet tractable, functions.
\par
On the other hand, we showed that MUEs learn a lower dimensional unitary from eigenvalue data of the unitary being studied. Which then can reduces Trotter error by extrapolating to small $dt$.
\par
Further investigations includes non-affine MMs, and extending MMs to approximate functions of matrices, such as matrix exponential, matrix logarithm, and others. A potential utilization of MUE, could be re-implementation of the lower dimensional $M(dt)$ back into the quantum circuit, reducing the number of quibts required for calculation. Given the number of physical quibts required to form a logical quibt through use of error correction code, MUEs as a quantum computing emulator for a specific calculation, might allow NISQ computers to do meaningful calculations. Additionally, the matrix elements in MM/MEEs could be further generalized, transforming them into functions or neural networks (NN). This adaptation enhances NN prowess through its elevation to an MM/MEEs, giving access to MMs/MEEs inherent structural framework. In a different field, MM could be beneficial for the Galerkin method used for solving partial differential equations. Lastly, we see application of MM to nuclear physics calculations, and other scientific problems.
\par
Initially, MM was proposed for applications of machine learning to nuclear theory. However, they can also be viewed as a new implicit deep learning architecture based on the generalization of eigenvector continuation and subspace projection methods now widely used in nuclear theory. Ultimately, our exploration of MM highlights their potential as a powerful tool for diverse applications.


%\bibliography{references}
\bibliographystyle{plain}
\bibliography{references}
\newpage

\appendix
\section{Material}
\subsection{TBS LMG}\label{TBS}
Also known as the Dicke basis, the TSB basis vectors are denoted by $\ket{S, M}$ where $S$ is the total spin of the system and $M$ is the projection of spin onto the $z$-axis. The LMG Hamiltonian preserves $S$.
In this basis, the LMG model Hamiltonian is given by
\begin{equation}
    H(c) = -S_z - \frac{2c}{N}\left(S_x^2 + S_y^2\right)
\end{equation}
This Hamiltonian exhibits the same phase transition at $c=1/2$.

For $N$ spins in the highest total spin sector ($S = N/2$), the size of the Hamiltonian in this basis is $(N+1)\times(N+1)$.
\subsection{Branch point analysis}\label{BPA}
\cite{srinivasan2020sorting}
 As stated in section 2, MMEs and MEEs exhibit the unique ability to discern branch points within the complex plane, a feat that is inaccessible to alternative methods. For instance, a polynomial could potentially generate an avoided level crossing for energy functions in relation to $c$, however, it would lack information concerning the branch points in the complex plain.
 \begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/bp.png}
    \caption{ A branch cut with a branch point at $x=0$ for original system Eq.~\ref{hbp}. Trained minimum eigenvalue of $\nxn{5}$ MEE on data only from the real line (x points), and extrapolated to the complex plain for complex values of $c$.\cite{srinivasan2020sorting}}
    \label{fig:bp}
\end{figure}
Consider the family of Hamiltonians:
 \begin{equation}
     \begin{aligned}
     \label{hbp}
         H = H_0 + \left(x-c\right)H_I
     \end{aligned}
 \end{equation}
 Where $x,c\in\mathbb{C}$ and $x$ is a fixed value. Through selection of $x$, we can construct a Hamiltonian that demonstrates an avoided level crossing near $c=x$.  
 \par
 As illustrated by a branch cut in Figure~\ref{fig:bp}, we trained the minimum eigenvalue of a real affine $\nxn{5}$ MEE on real values of $c$. We then extrapolated these results to the complex plane for complex values of $c$. The real affine MEE, despite trained only on real values of $c$, was able to gain insights on data for complex values of $c$, thus identifying the branch points of the original system. This implies that the general MEE structure can learn about these branch points without explicit data concerning them or data for complex $c$ values. In contrast, other methods, such as polynomial fits, often fail to learn about features in the complex plane when trained solely on the real line.
 \subsection{Heisenberg Model}\label{heisen}
 \begin{equation}
     H = h\sum_i^N\sigma_i^z + J\sum_i^N(\sigma_i^x\sigma_{i+1}^x +\sigma_i^y\sigma_{i+1}^y +\sigma_i^z\sigma_{i+1}^z) 
 \end{equation}
\section{Optimization}
\subsection{Speed}
From testing, we observed that training MMEs is capable of identifying effective solutions, particularly for smaller MMEs. With that knowledge, We designed an optimization scheme called progressive optimization, which uses this information to progressively build larger MMEs. Suppose we want an $n\times n$ MEE, we start by training the simple $\nxn{1}$ MME given by
\begin{equation}
    M_1(c) = a_1 + c b_1.
\end{equation}
We then use the result of training $a_1$ and $b_1$ to form the initial guess of the training for a $\nxn{2}$ MME:
\begin{equation}
    A_2^{(0)} = \begin{bmatrix}
        a_1 & 0 \\
        0 & \mathcal{R}_1
    \end{bmatrix},\quad
    B_2^{(0)} = \begin{bmatrix}
        b_1 & \mathcal{R}_2 \\
        \mathcal{R}_2 & \mathcal{R}_3
    \end{bmatrix},
\end{equation}
where $\mathcal{R}_i$ are random numbers. The results of training this $\nxn{2}$ MME are then used to form the initial guess for the next larger MME, and so on until the desired $\nxn{n}$ MME is trained. So long as the values from the previous MME are near the true optimal values, this reduces the effective number of search directions for the optimization algorithm from $n(n+3)/2$ to a maximum of $n+1$ during the final optimization.
\subsection{Minimization and Error Propagation}
In the case of MME, we the fact that $x$ is the parameter vector of a matrix whose spectrum defines $\phi(\cdot)$. Thus, instead of regularizing against the norm of the parameter vector, we instead regularize against
the average some matrix norm of the MME across all training points.
\begin{align}
    &x^* = \underset{x}{\arg\min}\frac{1}{N} \sum_i^N||M(x,c_i)||^2\\
    &\text{Subject to} \hspace{0.1cm} |\phi(x)-y_i|\leq \epsilon \hspace{0.1cm}\forall i
\end{align}
where $||\cdot||^2$ is some consistent matrix norm, in this note we use the Frobenius norm.
The second problem is addressed by the introduction of a slack variable si for each training point, $s = [s_0, s_1, \cdots , s_n]$,
which allow each constraint to be broken by an amount no greater than $|s_i|$. The minimization problem then becomes
\begin{align}
    &x^* = \underset{x}{\arg\min}\frac{1}{N} \sum_i^N||M(x,c_i)||^2 +\gamma ||s||^2\\
    &\text{Subject to} \hspace{0.1cm} |\phi(x)-y_i|\leq \epsilon +|s_i| \hspace{0.1cm}\forall i
\end{align}
Where $\gamma$ is a hyperparameter which defines the relative importance of the slack variable penalty term, which is a vector norm. \\
By utilizing completely unconstrained minimization with Lagrange multipliers, we can get an SVR-like cost
function which contains all the information necessary to perform one-shot uncertainty quantification via the usual inverse
Hessian approach. As the technique of Lagrange multipliers is covered in many numerical analysis textbooks, we provide
only the final formulation of the problem below
\begin{align}
\label{cost}
    x^* = \underset{x}{\arg\min}\frac{1}{N} \sum_i^N||M(x,c_i)||^2 +\gamma ||s||^2 +\beta\sum_i^N|\phi(x)-y_i| -\epsilon +|s_i|
\end{align}
Where $\beta$ is a hyperparameter which defines the relative importance of the Lagrange multiplier penalty term.\\
Denote the cost function in Eq. \ref{cost} by $f(X)$. Uncertainties in any arbitrary function of the parameters $X=[x,s]$ can be calculated by the usual error propagation formula
\begin{align}
    \Delta \theta^2 = (f(X)-\hat{f})\sum_{ij} \left(\frac{\partial\theta}{\partial X_i}\bigg|_{X^*} (H^{-1})_{ij} \frac{\partial\theta}{\partial X_j}\bigg|_{X^*}\right)
\end{align}
where
\begin{align}
    H_{ij} = \frac{1}{2}\frac{\partial^2 f}{\partial X_i\partial X_j} \bigg|_{X^*}
\end{align}
and $\hat{f}$ is the true global minimum of the cost fucntion. When the MME regularization term is the Forbenius norm, this is estimated as
\begin{align}
    \hat{f} \approx \frac{1}{N}\sum_i^N |y_i|^2-N\beta\epsilon
\end{align}
which assumes that $s^*=0$, the model fits the training data exactly, and uses the lower bound for the Forbenius norm of a Matrix A with eigenvalues $\lambda_i$
\begin{align}
    \sum_i|\lambda_i|^2\leq||A||_F^2
\end{align}
\subsection{Hellmann-Feynman Theorem}
When training MMEs, all computational tasks are handled by local optimization algorithms, including but not limited to Nelder-Mead, L-BFGS-B, Powell, and Newton-CG. For detailed information on how these optimizers are used, please refer to our GitHub package. This also includes the computation of the gradient of the MME. However, if one wants to use an analytical gradient, one can  use the Hellmann-Feynman Theorem (HFT). HFT relates the derivative of an eigenvalue of $M(\alpha)$ for some $\alpha$ to the expectation value of:
\begin{equation}
    \begin{aligned}
        \frac{dE}{d\alpha} = \bra{\psi_\alpha}\frac{dM(\alpha)}{d\alpha}\ket{\psi_\alpha}
    \end{aligned}
\end{equation}
for the corresponding $k^{th}$ eigenvector $\ket{\psi_\alpha}$ of $M(\alpha)$, so long as $\frac{d}{d\alpha}\braket{\psi_\alpha}{\psi_\alpha}=0$. Here, we have $\alpha =a_{ij},b_{ij}$ the matrix elements of $A$ and $B$ of $M$. Due to our choice of $M(c)$, $\frac{dM(c)}{d\alpha_i}$ can be readily calculated as:
\begin{equation}
    \label{eq:partialM}
    \frac{\partial M(c)}{\partial \alpha_j} = \begin{cases}
        \mathbf{e}_j \mathbf{e}_j^T & \text{if } (\alpha_j~\text{is in}~diag(A)) \\
        c\left(\mathbf{e}_j \mathbf{e}_j^T\right) & \text{if } (\alpha_j~\text{is in}~diag(B))\\
        c\left(\mathbf{e}_j \mathbf{e}_i^T + \mathbf{e}_i \mathbf{e}_j^T\right) & \text{if }  (\alpha_j~\text{is in off-}diag(B))
    \end{cases}.
\end{equation}
Where we have switched to zero-based indexing for the vectors, and $\mathbf{e}_i$ is the $n$-dimensional column vector of all zeros except for a single $1$ at the $i^\mathrm{th}$ position. 
%########################################
\end{document}
